{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamamh66/Explainable-Multi-Agent-Energy-Twin-for-Resilient-and-Carbon-Aware-Grid-Scheduling/blob/main/QFF_Improved_Federated_VQC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7TsUojjMNUH"
      },
      "source": [
        "# Quantum-Federated Forecasting (QFF) with VQC-Inspired Encoding\n",
        "\n",
        "Clean, documented notebook for:\n",
        "\n",
        "- Loading ISO-NE hourly data (2024)\n",
        "- Building supervised 168→24 forecasting windows\n",
        "- Training centralized baselines: LSTM, TCN, Transformer\n",
        "- Training a federated QFF model (VQC-inspired encoder + LSTM head)\n",
        "- Evaluating MAE, RMSE, MAPE on the test set\n",
        "- Plotting seasonal error profiles\n",
        "- Plotting capacity-margin curves for a representative 24 h horizon\n",
        "\n",
        "**Note:**\n",
        "- This notebook assumes that the CSV file `2024_smd_hourly.csv` is already in your Google Drive.\n",
        "- You may need to adapt the column names in the data loading cell to match your file.\n"
      ],
      "id": "w7TsUojjMNUH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-z4FiapMNUK"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Install / import all required libraries\n",
        "\n",
        "!pip install tensorflow-addons -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ],
      "id": "z-z4FiapMNUK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uI-LCSvMNUM"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# (Optional) Mount Google Drive when running in Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Base directory in your Drive where data and figures will be saved\n",
        "BASE_DIR = \"/content/drive/MyDrive/QFF/\"  # adapt if needed\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Path to ISO-NE hourly CSV (already downloaded from ISO-NE)\n",
        "DATA_PATH = os.path.join(BASE_DIR, \"2024_smd_hourly.csv\")  # adapt file name if needed\n",
        "print(\"Data path:\", DATA_PATH)\n"
      ],
      "id": "1uI-LCSvMNUM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdi74PdCMNUM"
      },
      "source": [
        "## 1. Data Loading and Preprocessing\n",
        "\n",
        "We assume an hourly time series for one zone (or aggregated system):\n",
        "\n",
        "- A timestamp (local or UTC)\n",
        "- A target column (e.g., load or price)\n",
        "- Optionally zone/aggregation columns\n",
        "\n",
        "Adapt the column names in the next cell to match your CSV.\n"
      ],
      "id": "Kdi74PdCMNUM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFJwZLfcMNUN"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Load ISO-NE hourly data and build a single time series\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Raw columns:\\n\", df.columns.tolist())\n",
        "print(\"First 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "# === ADAPT THIS BLOCK AS NEEDED ===\n",
        "# Try to identify a timestamp column and a numerical target column.\n",
        "# Example patterns (uncomment / adapt depending on your CSV):\n",
        "\n",
        "# 1) If you already have a combined datetime column, e.g. 'Datetime':\n",
        "# df[\"timestamp\"] = pd.to_datetime(df[\"Datetime\"])\n",
        "\n",
        "# 2) If you have 'Local Date' and 'Local Hour Ending' columns:\n",
        "# df[\"timestamp\"] = pd.to_datetime(df[\"Local Date\"] + \" \" + df[\"Local Hour Ending\"].astype(str) + \":00\")\n",
        "\n",
        "# 3) Generic fallback: try to infer datetime from a column named 'Date' or 'Date/Time':\n",
        "if \"timestamp\" not in df.columns:\n",
        "    for cand in [\"Date/Time\", \"Datetime\", \"DateTime\", \"Date\", \"DATE\"]:\n",
        "        if cand in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_datetime(df[cand])\n",
        "            break\n",
        "\n",
        "if \"timestamp\" not in df.columns:\n",
        "    raise ValueError(\"Please define df['timestamp'] from your date/hour columns.\")\n",
        "\n",
        "    # Choose a numeric target column (e.g., system load or price). Replace this with your actual column.\n",
        "# Examples: 'RT_Demand_MW', 'DA_Demand_MW', 'Load', 'VALUE'.\n",
        "TARGET_COL = \"Load\"  # TODO: adapt to your actual target column name\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TARGET_COL}' not found. Please set TARGET_COL to a valid numeric column.\")\n",
        "\n",
        "# Keep only timestamp and target for now\n",
        "df = df[[\"timestamp\", TARGET_COL]].copy()\n",
        "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "print(\"Data after selection:\")\n",
        "display(df.head())\n",
        "print(\"Number of rows:\", len(df))\n"
      ],
      "id": "vFJwZLfcMNUN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ5n786PMNUN"
      },
      "source": [
        "### 1.1 Feature Engineering\n",
        "\n",
        "We derive simple calendar features from the timestamp:\n",
        "\n",
        "- Hour of day\n",
        "- Day of week\n",
        "- Month of year\n",
        "- (Optionally) day of year\n"
      ],
      "id": "nJ5n786PMNUN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNl1zdQpMNUO"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Build calendar features\n",
        "\n",
        "df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "df[\"dow\"] = df[\"timestamp\"].dt.dayofweek  # Monday=0\n",
        "df[\"month\"] = df[\"timestamp\"].dt.month\n",
        "df[\"doy\"] = df[\"timestamp\"].dt.dayofyear\n",
        "\n",
        "# Reorder columns: timestamp, target, then features\n",
        "cols = [\"timestamp\", TARGET_COL, \"hour\", \"dow\", \"month\", \"doy\"]\n",
        "df = df[cols]\n",
        "\n",
        "display(df.head())\n"
      ],
      "id": "kNl1zdQpMNUO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpP55da_MNUP"
      },
      "source": [
        "### 1.2 Scaling and Supervised Windowing (168→24)\n",
        "\n",
        "We convert the time series into supervised learning windows:\n",
        "\n",
        "- Input window length: 168 hours (7 days)\n",
        "- Forecast horizon: 24 hours ahead\n",
        "\n",
        "Features are standardized; the target is also scaled but later inverse-transformed for metrics and plots.\n"
      ],
      "id": "LpP55da_MNUP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkiUR7lkMNUP"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "LOOKBACK_H = 168   # input window size\n",
        "HORIZON_H  = 24    # output horizon\n",
        "\n",
        "# Feature matrix (excluding timestamp)\n",
        "feature_cols = [TARGET_COL, \"hour\", \"dow\", \"month\", \"doy\"]\n",
        "X_raw = df[feature_cols].values.astype(np.float32)\n",
        "y_raw = df[TARGET_COL].values.astype(np.float32)\n",
        "\n",
        "# Scale features and target separately\n",
        "x_scaler = StandardScaler()\n",
        "X_scaled = x_scaler.fit_transform(X_raw)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_scaled = y_scaler.fit_transform(y_raw.reshape(-1, 1)).ravel()\n",
        "\n",
        "timestamps = df[\"timestamp\"].values\n",
        "\n",
        "def build_supervised_sequences(X, y, ts, lookback=168, horizon=24):\n",
        "    \"\"\"Build (X_seq, Y_seq, TS_targets) for sliding-window forecasting.\"\"\"\n",
        "    X_seqs = []\n",
        "    Y_seqs = []\n",
        "    TS_targets = []\n",
        "\n",
        "    n = len(y)\n",
        "    max_start = n - lookback - horizon + 1\n",
        "    for start in range(max_start):\n",
        "        end_in = start + lookback\n",
        "        end_out = end_in + horizon\n",
        "\n",
        "        X_win = X[start:end_in]\n",
        "        Y_win = y[end_in:end_out]\n",
        "        TS_win = ts[end_in:end_out]\n",
        "\n",
        "        X_seqs.append(X_win)\n",
        "        Y_seqs.append(Y_win)\n",
        "        TS_targets.append(TS_win)\n",
        "\n",
        "    return (\n",
        "        np.array(X_seqs, dtype=np.float32),\n",
        "        np.array(Y_seqs, dtype=np.float32),\n",
        "        np.array(TS_targets)\n",
        "    )\n",
        "\n",
        "X_seq, Y_seq, TS_targets = build_supervised_sequences(\n",
        "    X_scaled, y_scaled, timestamps,\n",
        "    lookback=LOOKBACK_H, horizon=HORIZON_H\n",
        ")\n",
        "\n",
        "print(\"X_seq shape:\", X_seq.shape)      # (n_samples, 168, n_features)\n",
        "print(\"Y_seq shape:\", Y_seq.shape)      # (n_samples, 24)\n",
        "print(\"TS_targets shape:\", TS_targets.shape)  # (n_samples, 24)\n"
      ],
      "id": "xkiUR7lkMNUP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_65fz6GMNUQ"
      },
      "source": [
        "### 1.3 Train/Validation/Test Split\n",
        "\n",
        "We perform a chronological split of the supervised sequences. For example:\n",
        "\n",
        "- 70% training\n",
        "- 15% validation\n",
        "- 15% testing\n"
      ],
      "id": "h_65fz6GMNUQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5HE44CVMNUQ"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "n_samples = X_seq.shape[0]\n",
        "train_end = int(0.7 * n_samples)\n",
        "val_end   = int(0.85 * n_samples)\n",
        "\n",
        "X_train, Y_train, TS_train = X_seq[:train_end], Y_seq[:train_end], TS_targets[:train_end]\n",
        "X_val,   Y_val,   TS_val   = X_seq[train_end:val_end], Y_seq[train_end:val_end], TS_targets[train_end:val_end]\n",
        "X_test,  Y_test,  TS_test  = X_seq[val_end:], Y_seq[val_end:], TS_targets[val_end:]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
        "print(\"Val shape:  \", X_val.shape,   Y_val.shape)\n",
        "print(\"Test shape: \", X_test.shape,  Y_test.shape)\n",
        "\n",
        "n_features = X_train.shape[2]\n",
        "input_len  = LOOKBACK_H\n",
        "output_len = HORIZON_H\n"
      ],
      "id": "q5HE44CVMNUQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADP1x7JmMNUS"
      },
      "source": [
        "## 2. Centralized Baseline Models\n",
        "\n",
        "We define three baseline architectures:\n",
        "\n",
        "- LSTM-based sequence model\n",
        "- TCN (Temporal Convolutional Network)\n",
        "- Transformer-style encoder\n",
        "\n",
        "All models take an input of shape `(lookback, n_features)` and output a vector of length `HORIZON_H`.\n"
      ],
      "id": "ADP1x7JmMNUS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZghspWMNUT"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "def build_lstm_model(input_len, n_features, output_len):\n",
        "    inputs = keras.Input(shape=(input_len, n_features))\n",
        "    x = layers.LSTM(64, return_sequences=True)(inputs)\n",
        "    x = layers.LSTM(32)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(output_len)(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"lstm_baseline\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_tcn_model(input_len, n_features, output_len):\n",
        "    inputs = keras.Input(shape=(input_len, n_features))\n",
        "    x = layers.Conv1D(64, kernel_size=3, padding=\"causal\", activation=\"relu\")(inputs)\n",
        "    x = layers.Conv1D(64, kernel_size=3, padding=\"causal\", activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(output_len)(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"tcn_baseline\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_transformer_encoder(input_len, n_features, d_model=64, num_heads=2, ff_dim=128, num_layers=2):\n",
        "    \"\"\"Simple Transformer encoder stack returning a sequence output.\"\"\"\n",
        "    inputs = keras.Input(shape=(input_len, n_features))\n",
        "    # Linear projection to d_model\n",
        "    x = layers.Dense(d_model)(inputs)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        # Multi-head self-attention block\n",
        "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
        "\n",
        "        # Feed-forward block\n",
        "        ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "        ff = layers.Dense(d_model)(ff)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "\n",
        "    return inputs, x\n",
        "\n",
        "def build_transformer_model(input_len, n_features, output_len):\n",
        "    inputs, encoded = build_transformer_encoder(input_len, n_features)\n",
        "    x = layers.GlobalAveragePooling1D()(encoded)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(output_len)(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"transformer_baseline\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"Models defined.\")\n"
      ],
      "id": "6VZghspWMNUT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzN1OyH2MNUV"
      },
      "source": [
        "### 2.1 Training Centralized Baselines\n",
        "\n",
        "We train each baseline for a moderate number of epochs with early stopping on the validation loss.\n"
      ],
      "id": "wzN1OyH2MNUV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJ97ftXMNUW"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"Training centralized LSTM...\")\n",
        "lstm_model = build_lstm_model(input_len, n_features, output_len)\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training centralized TCN...\")\n",
        "tcn_model = build_tcn_model(input_len, n_features, output_len)\n",
        "history_tcn = tcn_model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training centralized Transformer...\")\n",
        "transformer_model = build_transformer_model(input_len, n_features, output_len)\n",
        "history_tr = transformer_model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Baseline training complete.\")\n"
      ],
      "id": "MTJ97ftXMNUW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OogpbI0kMNUX"
      },
      "source": [
        "## 3. Federated QFF Model with VQC-Inspired Encoding\n",
        "\n",
        "We now define the QFF model:\n",
        "\n",
        "- Input: `(lookback, n_features)`\n",
        "- VQC-inspired encoder:\n",
        "  - Time-distributed dense projection to a higher-dimensional latent space (\"parameterized rotations\")\n",
        "  - Sine/cosine mixing to emulate interference-like behavior\n",
        "- LSTM head for temporal forecasting\n",
        "- Output: 24-step horizon\n",
        "\n",
        "Federated learning is simulated by partitioning the training data into `K` nodes and performing a simple FedAvg loop over `R` rounds.\n"
      ],
      "id": "OogpbI0kMNUX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNBTxjc9MNUY"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "def vqc_encoder_block(inputs, d_model=64, depth=2):\n",
        "    \"\"\"Simple VQC-inspired encoder using TimeDistributed dense + sine/cosine mixing.\"\"\"\n",
        "    x = inputs\n",
        "    for _ in range(depth):\n",
        "        # Parameterized linear map\n",
        "        x_lin = layers.TimeDistributed(layers.Dense(d_model))(x)\n",
        "        # Nonlinear phase-like transformation\n",
        "        x_sin = tf.sin(x_lin)\n",
        "        x_cos = tf.cos(x_lin)\n",
        "        # Concatenate and project back to d_model\n",
        "        x = layers.Concatenate(axis=-1)([x_sin, x_cos])\n",
        "        x = layers.TimeDistributed(layers.Dense(d_model, activation=\"relu\"))(x)\n",
        "    return x\n",
        "\n",
        "def build_qff_vqc_model(input_len, n_features, output_len, d_model=64, depth=2):\n",
        "    inputs = keras.Input(shape=(input_len, n_features))\n",
        "    # VQC-inspired encoder\n",
        "    z = vqc_encoder_block(inputs, d_model=d_model, depth=depth)\n",
        "    # LSTM head\n",
        "    h = layers.LSTM(64, return_sequences=True)(z)\n",
        "    h = layers.LSTM(32)(h)\n",
        "    h = layers.Dense(64, activation=\"relu\")(h)\n",
        "    outputs = layers.Dense(output_len)(h)\n",
        "    model = keras.Model(inputs, outputs, name=\"qff_vqc_model\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"QFF model definition ready.\")\n"
      ],
      "id": "gNBTxjc9MNUY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uijgXHLMNUZ"
      },
      "source": [
        "### 3.1 Simple Federated Averaging (FedAvg) Simulation\n",
        "\n",
        "We split the training data into `K` nodes and perform `R` federated rounds:\n",
        "\n",
        "For each round:\n",
        "1. Broadcast global weights to each node model.\n",
        "2. Train locally on each node for a small number of epochs.\n",
        "3. Collect updated weights from each node.\n",
        "4. Average weights (FedAvg) to obtain the new global model.\n"
      ],
      "id": "9uijgXHLMNUZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ero_E_KYMNUZ"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "def get_federated_splits(X, Y, K=3):\n",
        "    \"\"\"Split (X, Y) along sample dimension into K shards for federated nodes.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    splits_X = np.array_split(X, K, axis=0)\n",
        "    splits_Y = np.array_split(Y, K, axis=0)\n",
        "    return list(zip(splits_X, splits_Y))\n",
        "\n",
        "def average_weights(weight_list):\n",
        "    \"\"\"Average a list of weight lists (FedAvg).\"\"\"\n",
        "    avg = []\n",
        "    for weights in zip(*weight_list):\n",
        "        w_stack = np.stack(weights, axis=0)\n",
        "        avg.append(np.mean(w_stack, axis=0))\n",
        "    return avg\n",
        "\n",
        "# Federated configuration\n",
        "K_NODES = 3\n",
        "R_ROUNDS = 5\n",
        "LOCAL_EPOCHS = 1\n",
        "LOCAL_BATCH = 64\n",
        "\n",
        "fed_splits = get_federated_splits(X_train, Y_train, K=K_NODES)\n",
        "for i, (Xk, Yk) in enumerate(fed_splits):\n",
        "    print(f\"Node {i}: X shape = {Xk.shape}, Y shape = {Yk.shape}\")\n",
        "\n",
        "print(\"Training federated QFF (VQC-inspired)...\")\n",
        "qff_model_global = build_qff_vqc_model(input_len, n_features, output_len, d_model=64, depth=2)\n",
        "\n",
        "for r in range(R_ROUNDS):\n",
        "    print(f\"\\n[Round {r+1}/{R_ROUNDS}]\")\n",
        "    node_weights = []\n",
        "    for node_id, (Xk, Yk) in enumerate(fed_splits):\n",
        "        print(f\"  Node {node_id}: local training...\")\n",
        "        # Clone global model for local node\n",
        "        node_model = build_qff_vqc_model(input_len, n_features, output_len, d_model=64, depth=2)\n",
        "        node_model.set_weights(qff_model_global.get_weights())\n",
        "\n",
        "        node_model.fit(\n",
        "            Xk, Yk,\n",
        "            epochs=LOCAL_EPOCHS,\n",
        "            batch_size=LOCAL_BATCH,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        node_weights.append(node_model.get_weights())\n",
        "\n",
        "    # FedAvg\n",
        "    new_global_weights = average_weights(node_weights)\n",
        "    qff_model_global.set_weights(new_global_weights)\n",
        "\n",
        "print(\"Federated QFF training complete.\")\n"
      ],
      "id": "ero_E_KYMNUZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLJrkHfMNUZ"
      },
      "source": [
        "## 4. Evaluation on Test Set\n",
        "\n",
        "We compute MAE, RMSE, and MAPE on the **inverse-scaled** test predictions for all models.\n"
      ],
      "id": "EqLJrkHfMNUZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5avlNu4fMNUZ"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "def mae(y_true, y_pred):\n",
        "    return float(mean_absolute_error(y_true, y_pred))\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    eps = 1e-6\n",
        "    return float(np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100.0)\n",
        "\n",
        "def inverse_targets(y_scaled_seq):\n",
        "    flat = y_scaled_seq.reshape(-1, 1)\n",
        "    inv_flat = y_scaler.inverse_transform(flat).ravel()\n",
        "    return inv_flat.reshape(y_scaled_seq.shape)\n",
        "\n",
        "# 1) Get scaled predictions on the test set\n",
        "Y_pred_lstm_scaled = lstm_model.predict(X_test, verbose=0)\n",
        "Y_pred_tcn_scaled  = tcn_model.predict(X_test, verbose=0)\n",
        "Y_pred_tr_scaled   = transformer_model.predict(X_test, verbose=0)\n",
        "Y_pred_qff_scaled  = qff_model_global.predict(X_test, verbose=0)\n",
        "\n",
        "# 2) Inverse scale\n",
        "Y_test_inv      = inverse_targets(Y_test)\n",
        "Y_pred_lstm_inv = inverse_targets(Y_pred_lstm_scaled)\n",
        "Y_pred_tcn_inv  = inverse_targets(Y_pred_tcn_scaled)\n",
        "Y_pred_tr_inv   = inverse_targets(Y_pred_tr_scaled)\n",
        "Y_pred_qff_inv  = inverse_targets(Y_pred_qff_scaled)\n",
        "\n",
        "# 3) Flatten for global metrics\n",
        "Y_true_flat = Y_test_inv.reshape(-1)\n",
        "Y_lstm_flat = Y_pred_lstm_inv.reshape(-1)\n",
        "Y_tcn_flat  = Y_pred_tcn_inv.reshape(-1)\n",
        "Y_tr_flat   = Y_pred_tr_inv.reshape(-1)\n",
        "Y_qff_flat  = Y_pred_qff_inv.reshape(-1)\n",
        "\n",
        "metrics = {}\n",
        "for name, yhat in [\n",
        "    (\"QFF\", Y_qff_flat),\n",
        "    (\"Transformer\", Y_tr_flat),\n",
        "    (\"LSTM\", Y_lstm_flat),\n",
        "    (\"TCN\", Y_tcn_flat),\n",
        "]:\n",
        "    metrics[name] = {\n",
        "        \"MAE\": mae(Y_true_flat, yhat),\n",
        "        \"RMSE\": rmse(Y_true_flat, yhat),\n",
        "        \"MAPE\": mape(Y_true_flat, yhat),\n",
        "        \"CRPS\": None\n",
        "    }\n",
        "\n",
        "print(\"Test metrics (inverse-scaled):\")\n",
        "for name, m in metrics.items():\n",
        "    print(\n",
        "        f\"{name}: MAE={m['MAE']:.3f}, RMSE={m['RMSE']:.3f}, MAPE={m['MAPE']:.3f}\"\n",
        "    )\n"
      ],
      "id": "5avlNu4fMNUZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20SWLBmnMNUa"
      },
      "source": [
        "## 5. Seasonal Error Profiles (Full Year)\n",
        "\n",
        "We now compute seasonal MAE (Winter, Spring, Summer, Fall) over the *entire* test horizon and produce a grouped bar chart.\n"
      ],
      "id": "20SWLBmnMNUa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGBCOoCIMNUa"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "def get_season(dt):\n",
        "    \"\"\"Simple season mapping for Northern Hemisphere.\"\"\"\n",
        "    m = dt.month\n",
        "    if m in [12, 1, 2]:\n",
        "        return \"Winter\"\n",
        "    elif m in [3, 4, 5]:\n",
        "        return \"Spring\"\n",
        "    elif m in [6, 7, 8]:\n",
        "        return \"Summer\"\n",
        "    else:\n",
        "        return \"Fall\"\n",
        "\n",
        "def seasonal_mae_flat(y_true_flat, y_pred_flat, ts_flat):\n",
        "    seasons = pd.Series(ts_flat).apply(get_season)\n",
        "    df_tmp = pd.DataFrame({\n",
        "        \"y\": y_true_flat,\n",
        "        \"yhat\": y_pred_flat,\n",
        "        \"season\": seasons.values\n",
        "    })\n",
        "    out = df_tmp.groupby(\"season\").apply(\n",
        "        lambda g: mean_absolute_error(g[\"y\"], g[\"yhat\"])\n",
        "    )\n",
        "    return out.reindex([\"Winter\", \"Spring\", \"Summer\", \"Fall\"])\n",
        "\n",
        "# Flatten timestamps of the test targets\n",
        "TS_test_flat = TS_test.reshape(-1)\n",
        "\n",
        "mae_qff  = seasonal_mae_flat(Y_true_flat, Y_qff_flat, TS_test_flat)\n",
        "mae_lstm = seasonal_mae_flat(Y_true_flat, Y_lstm_flat, TS_test_flat)\n",
        "mae_tcn  = seasonal_mae_flat(Y_true_flat, Y_tcn_flat,  TS_test_flat)\n",
        "mae_tr   = seasonal_mae_flat(Y_true_flat, Y_tr_flat,   TS_test_flat)\n",
        "\n",
        "print(\"Seasonal MAE (MW) on test set:\")\n",
        "print(\"QFF:\\n\", mae_qff)\n",
        "print(\"LSTM:\\n\", mae_lstm)\n",
        "print(\"TCN:\\n\", mae_tcn)\n",
        "print(\"Transformer:\\n\", mae_tr)\n",
        "\n",
        "models = [\"QFF\", \"LSTM\", \"TCN\", \"Transformer\"]\n",
        "seasons_order = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
        "mae_values = np.array([\n",
        "    mae_qff.values,\n",
        "    mae_lstm.values,\n",
        "    mae_tcn.values,\n",
        "    mae_tr.values\n",
        "])\n",
        "\n",
        "x = np.arange(len(seasons_order))\n",
        "width = 0.2\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, model in enumerate(models):\n",
        "    plt.bar(x + i*width, mae_values[i], width=width, label=model)\n",
        "\n",
        "plt.xticks(x + 1.5*width, seasons_order)\n",
        "plt.ylabel(\"MAE (MW)\")\n",
        "plt.title(\"Seasonal Error Profiles — QFF vs Centralized Baselines (Test Set)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.legend()\n",
        "\n",
        "fig1_path = os.path.join(BASE_DIR, \"fig_seasonal_error_profiles.pdf\")\n",
        "plt.savefig(fig1_path, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved seasonal error profile figure to:\", fig1_path)\n"
      ],
      "id": "JGBCOoCIMNUa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqD3B1kyMNUa"
      },
      "source": [
        "## 6. Capacity-Margin Curves for a Representative 24-Hour Horizon\n",
        "\n",
        "We define a simple capacity margin model:\n",
        "\n",
        "- Choose one test sample (24-hour horizon)\n",
        "- Use a constant capacity `C_t = C` chosen slightly above the observed maximum for that day\n",
        "- Compute true margin `M_t = C_t - y_t` and QFF-based margin `\\hat{M}_t = C_t - \\hat{y}_t`\n",
        "- Plot both curves to visualize under/over-estimation patterns\n"
      ],
      "id": "aqD3B1kyMNUa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Nv_eD2MNUa"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Select a representative 24-hour horizon from the test set\n",
        "idx_sample = 0  # change to inspect another day\n",
        "\n",
        "y_true_day = Y_test_inv[idx_sample]       # shape (24,)\n",
        "y_qff_day  = Y_pred_qff_inv[idx_sample]   # shape (24,)\n",
        "ts_day     = TS_test[idx_sample]          # array of datetimes (24,)\n",
        "\n",
        "# Define a simple capacity level slightly above the daily max\n",
        "C_level = float(y_true_day.max() + 0.10 * (y_true_day.max() - y_true_day.min()))\n",
        "C_day = np.full_like(y_true_day, C_level)\n",
        "\n",
        "M_true = C_day - y_true_day\n",
        "M_qff  = C_day - y_qff_day\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(ts_day, M_true, label=\"True margin\", linewidth=2)\n",
        "plt.plot(ts_day, M_qff, label=\"QFF-based margin\", linestyle=\"--\", linewidth=2)\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Capacity margin (MW)\")\n",
        "plt.title(\"True vs QFF-based Capacity Margins (Representative 24 h)\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.legend()\n",
        "\n",
        "fig2_path = os.path.join(BASE_DIR, \"fig_capacity_margin_curves.pdf\")\n",
        "plt.savefig(fig2_path, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved capacity-margin figure to:\", fig2_path)\n"
      ],
      "id": "G7Nv_eD2MNUa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atk3waeLMNUb"
      },
      "source": [
        "## 7. Summary\n",
        "\n",
        "- The notebook builds a corrected 168→24 supervised pipeline for ISO-NE hourly data.\n",
        "- Centralized baselines (LSTM, TCN, Transformer) and a federated QFF model are trained.\n",
        "- Global MAE/RMSE/MAPE are reported on the inverse-scaled test set.\n",
        "- Seasonal MAE profiles and capacity-margin curves are generated as PDF figures in `BASE_DIR`.\n",
        "\n",
        "These results can be directly used to populate the quantitative comparison table and figures in the manuscript.\n"
      ],
      "id": "Atk3waeLMNUb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}